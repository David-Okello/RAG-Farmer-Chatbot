# === prod_rag.py ===
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict

# 1. Load & chunk documents (with metadata)
def load_and_chunk(docs_folder: str, chunk_size: int = 500, overlap: int = 100) -> List[Dict]:
    chunks = []
    for file in Path(docs_folder).glob("*.txt"):
        text = file.read_text()
        tokens = text.split()
        for i in range(0, len(tokens), chunk_size - overlap):
            chunk = " ".join(tokens[i : i + chunk_size])
            chunks.append({
                "text": chunk,
                "meta": {"source": file.name}
            })
    return chunks

# 2. Encode with sentence-transformers
model = SentenceTransformer("all-MiniLM-L6-v2")
docs = load_and_chunk("reports/") # Reports generated by create_reports.py
corpus_embeddings = model.encode([d["text"] for d in docs], show_progress_bar=True)

# 3. Build FAISS index
dim = corpus_embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
faiss.normalize_L2(corpus_embeddings)
index.add(corpus_embeddings)

# 4. Retrieval + prompt composition
def retrieve_and_prompt(query: str, top_k: int = 3, filter_meta: Dict = None) -> str:
    q_emb = model.encode([query])
    faiss.normalize_L2(q_emb)
    D, I = index.search(q_emb, top_k)
    retrieved = []
    for idx in I[0]:
        doc = docs[idx]
        if not filter_meta or all(doc["meta"].get(k) == v for k, v in filter_meta.items()):
            retrieved.append(doc["text"])
    context = "\n\n".join(f"{i+1}. {t}" for i, t in enumerate(retrieved))
    prompt = (
        "You are an agricultural expert chatbot. Use the contexts below:\n\n"
        f"{context}\n\nUser: {query}\nBot:"
    )
    return prompt

if __name__ == "__main__":
    print(retrieve_and_prompt("What soil health indicators should a farmer monitor?", top_k=3, filter_meta={"source": "east_africa_report.txt"}))
